import streamlit as st
import re
import PyPDF2
import os
import glob
import requests

# --- CONFIG VISUAL ---
st.set_page_config(page_title="Chatbot - Hojas de Seguridad", layout="wide", page_icon="üßë‚Äçüî¨")

st.markdown("""
    <style>
        body {
            background-color: #f3f6fa;
        }
        .main {
            background-color: #f9fafb;
        }
        .block-container {
            padding-top: 2rem;
        }
        .stTextInput>div>div>input {
            border-radius: 10px;
            border: 1px solid #009639;
        }
        .stButton>button {
            border-radius: 8px;
            background-color: #009639;
            color: white;
        }
        .stAlert {
            border-radius: 10px;
        }
        .st-bb {
            background: #e3f9ed;
            border-radius: 8px;
        }
        hr {
            border:1px solid #009639 !important;
        }
        .env-badge {
            background: #007bff;
            color: white;
            padding: 0.3em 0.8em;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: bold;
        }
        .local-badge {
            background: #28a745;
            color: white;
            padding: 0.3em 0.8em;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: bold;
        }
    </style>
""", unsafe_allow_html=True)

# --- CONFIG PATHS ---
DOCUMENTS_PATH = "documents"
LLAMA_SERVER_URL = "http://127.0.0.1:8000/completion"

# --- DETECTAR ENTORNO ---
def is_streamlit_cloud():
    """Detecta si est√° corriendo en Streamlit Cloud"""
    # Verificar variables de entorno espec√≠ficas de Streamlit Cloud
    cloud_indicators = [
        os.getenv("STREAMLIT_SHARING_MODE"),
        os.getenv("STREAMLIT_SERVER_PORT"),
        os.getenv("STREAMLIT_SERVER_ADDRESS")
    ]
    
    # Si alguna est√° presente, probablemente es cloud
    if any(cloud_indicators):
        return True
    
    # Verificar si estamos en un entorno tipo cloud (sin llama.cpp local)
    try:
        response = requests.get("http://127.0.0.1:8000", timeout=1)
        return False  # Si llama.cpp responde, estamos en local
    except:
        # Si no responde llama.cpp, probablemente estamos en cloud
        return True

# --- FUNCIONES LLM MISTRAL ---
def generate_ai_response(query, relevant_chunks):
    if not relevant_chunks:
        return "No se encontr√≥ informaci√≥n relevante en los documentos disponibles."
    
    context = "\n".join([chunk['content'][:400] for chunk in relevant_chunks[:2]])
    
    # Detectar entorno y usar Mistral correspondiente
    if is_streamlit_cloud():
        return generate_mistral_api_response(query, context)
    else:
        return generate_local_mistral_response(query, context)

def generate_mistral_api_response(query, context):
    """Mistral via API para Streamlit Cloud"""
    try:
        # Verificar si tenemos la API key de forma segura
        try:
            api_key = st.secrets["MISTRAL_API_KEY"]
        except:
            return "‚ö†Ô∏è API Key de Mistral no configurada. Ve a Settings ‚Üí Secrets en Streamlit Cloud."
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "mistral-small-latest",
            "messages": [
                {
                    "role": "system", 
                    "content": "Eres un asistente especializado en hojas de seguridad. SOLO responde bas√°ndote en la informaci√≥n proporcionada. Si la informaci√≥n no est√° disponible, di claramente que no se encuentra en los documentos."
                },
                {
                    "role": "user", 
                    "content": f"""CONTEXTO DE DOCUMENTOS:
{context}

PREGUNTA: {query}

Responde √∫nicamente bas√°ndote en el contexto anterior:"""
                }
            ],
            "max_tokens": 150,
            "temperature": 0.3
        }
        
        with st.spinner("ü§ñ Consultando Mistral API..."):
            response = requests.post(
                "https://api.mistral.ai/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
        
        if response.status_code == 200:
            data = response.json()
            text = data['choices'][0]['message']['content'].strip()
            
            # Validar que no est√© vac√≠a
            if not text:
                return "No pude generar una respuesta clara basada en los documentos."
            
            return text
            
        elif response.status_code == 401:
            return "‚ùå Error de autenticaci√≥n con Mistral API. Verifica tu API key."
        elif response.status_code == 429:
            return "‚è±Ô∏è L√≠mite de consultas alcanzado. Intenta de nuevo en unos minutos."
        else:
            return f"‚ùå Error API Mistral: {response.status_code}"
            
    except requests.exceptions.Timeout:
        return "‚è±Ô∏è Timeout: Mistral API est√° tardando demasiado."
    except requests.exceptions.ConnectionError:
        return "‚ùå Error de conexi√≥n con Mistral API."
    except Exception as e:
        return f"‚ùå Error inesperado: {str(e)}"

def generate_local_mistral_response(query, context):
    """Mistral local via llama.cpp"""
    prompt = f"""Eres un asistente especializado en hojas de seguridad. SOLO puedes responder bas√°ndote en la informaci√≥n proporcionada a continuaci√≥n.

REGLAS IMPORTANTES:
- Si la informaci√≥n no est√° en el contexto, responde: "Esta informaci√≥n no se encuentra en los documentos disponibles."
- NO inventes ni agregues informaci√≥n que no est√© expl√≠citamente en el contexto
- Cita espec√≠ficamente de qu√© documento proviene la informaci√≥n cuando sea posible

CONTEXTO DE LOS DOCUMENTOS:
{context}

PREGUNTA: {query}

RESPUESTA (solo basada en el contexto anterior):"""

    try:
        with st.spinner("ü§ñ Consultando Mistral local..."):
            response = requests.post(
                LLAMA_SERVER_URL,
                json={
                    "prompt": prompt,
                    "n_predict": 150,
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "top_k": 20,
                    "repeat_penalty": 1.05,
                    "stop": ["</s>", "PREGUNTA:", "CONTEXTO:", "REGLAS:"]
                },
                timeout=60
            )
        
        if response.status_code == 200:
            data = response.json()
            text = data.get("content", "").strip()
            
            # Limpiar la respuesta
            if "RESPUESTA:" in text:
                text = text.split("RESPUESTA:")[-1].strip()
            
            # Validar que no est√© vac√≠a
            if not text:
                return "No pude generar una respuesta clara basada en los documentos disponibles."
                
            return text
            
        else:
            return f"‚ùå Error del servidor llama.cpp: {response.status_code}"
            
    except requests.exceptions.Timeout:
        return "‚è±Ô∏è Timeout: El servidor llama.cpp est√° tardando demasiado."
    except requests.exceptions.ConnectionError:
        return "‚ùå No se puede conectar al servidor llama.cpp local. Verifica que est√© corriendo en http://127.0.0.1:8000"
    except Exception as e:
        return f"‚ùå Error de conexi√≥n local: {str(e)}"

def validate_response_relevance(response, query_keywords, context_keywords):
    """Valida si la respuesta est√° relacionada con el contexto"""
    response_words = extract_keywords(response.lower())
    
    # Verificar que la respuesta contenga palabras del contexto
    context_overlap = len(set(response_words) & set(context_keywords))
    
    # Si no hay overlap significativo, es probable que sea una respuesta inventada
    if context_overlap < 2 and len(response_words) > 5:
        return False
    return True

# --- FUNCIONES PDF y b√∫squeda ---
def extract_text_from_pdf(file_path):
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text
    except Exception:
        return ""

def simple_chunking(text, source, chunk_size=400):
    chunks = []
    text = re.sub(r'\s+', ' ', text).strip()
    if len(text) < 50:
        return chunks
    overlap = 50
    start = 0
    while start < len(text):
        end = start + chunk_size
        if end < len(text):
            space_pos = text.rfind(' ', end - 50, end)
            if space_pos > start:
                end = space_pos
        chunk_text = text[start:end].strip()
        if len(chunk_text) > 50:
            chunks.append({
                'content': chunk_text,
                'source': source
            })
        start = end - overlap
        if start >= len(text) - 50:
            break
    return chunks

@st.cache_data
def load_documents():
    all_chunks = []
    if not os.path.exists(DOCUMENTS_PATH):
        os.makedirs(DOCUMENTS_PATH)
        return all_chunks
    
    pdf_files = glob.glob(os.path.join(DOCUMENTS_PATH, "*.pdf"))
    for file_path in pdf_files:
        filename = os.path.basename(file_path)
        text = extract_text_from_pdf(file_path)
        if text.strip():
            chunks = simple_chunking(text, filename)
            all_chunks.extend(chunks)
    return all_chunks

def extract_keywords(text):
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    words = [w for w in words if len(w) > 2]
    stop_words = {
        'que', 'para', 'con', 'por', 'una', 'del', 'las', 'los', 'este', 'esta',
        'como', 'ser', 'son', 'est√°', 'est√°n', 'fue', 'sido', 'tiene', 'tienen',
        'puede', 'pueden', 'debe', 'deben', 'hacer', 'hace', 'muy', 'm√°s', 'menos',
        'todo', 'todos', 'toda', 'todas', 'cada', 'algunos', 'algunas', 'otro', 'otra'
    }
    words = [w for w in words if w not in stop_words]
    return words

def hybrid_search(query, all_chunks, top_k=3, min_score=0.15):
    if not all_chunks:
        return []
    
    query_keywords = extract_keywords(query)
    results = []
    
    for chunk in all_chunks:
        chunk_text = chunk['content'].lower()
        chunk_keywords = extract_keywords(chunk_text)
        
        exact_matches = len(set(query_keywords) & set(chunk_keywords))
        exact_score = exact_matches / max(len(query_keywords), 1)
        
        partial_score = 0
        for qword in query_keywords:
            for cword in chunk_keywords:
                if qword in cword or cword in qword:
                    partial_score += 0.5
        partial_score = partial_score / max(len(query_keywords), 1)
        
        phrase_score = 0
        query_lower = query.lower()
        if len(query_lower) > 10:
            query_words = query_lower.split()
            for i in range(len(query_words) - 2):
                phrase = ' '.join(query_words[i:i+3])
                if phrase in chunk_text:
                    phrase_score += 1
        
        final_score = (
            exact_score * 0.5 +
            partial_score * 0.2 + 
            phrase_score * 0.3
        )
        
        if final_score >= min_score:
            results.append({
                'content': chunk['content'],
                'source': chunk['source'],
                'score': final_score,
                'exact_matches': exact_matches,
                'keywords': chunk_keywords,
                'debug': f"Exact:{exact_score:.2f} Partial:{partial_score:.2f} Phrase:{phrase_score:.2f}"
            })
    
    results.sort(key=lambda x: x['score'], reverse=True)
    return results[:top_k]

# --- INTERFAZ ---
# Detectar entorno para mostrar badge
is_cloud = is_streamlit_cloud()
env_badge = "‚òÅÔ∏è CLOUD (Mistral API)" if is_cloud else "üè† LOCAL (llama.cpp)"
badge_class = "env-badge" if is_cloud else "local-badge"

st.markdown(
    f"""
    <div style='text-align:center;'>
        <h1 style='color:#009639; font-size: 2.8rem; margin-bottom:0.2em;'>üßë‚Äçüî¨ Chatbot - Hojas de Seguridad</h1>
        <span class='{badge_class}'>{env_badge}</span>
    </div>
    <hr style='border:1px solid #009639; margin-top:1em; margin-bottom:1.5em;'/>
    """,
    unsafe_allow_html=True
)

all_chunks = load_documents()


if all_chunks:
    st.success(f"‚úÖ {len(all_chunks)} fragmentos de documentos cargados")
    
    query = st.text_input(
        "üí¨ Escribe tu pregunta sobre seguridad, aplicaci√≥n, dosis, etc.",
        placeholder="Ejemplo: ¬øQu√© hacer en caso de contacto con los ojos?",
        key="input_pregunta"
    )
    
    if query:
        results = hybrid_search(query, all_chunks, top_k=3, min_score=0.15)
        
        if results:
            ai_response = generate_ai_response(query, results)
            
            # Validaci√≥n adicional
            query_keywords = extract_keywords(query)
            context_keywords = []
            for result in results:
                context_keywords.extend(result.get('keywords', []))
            
            # Verificar si la respuesta parece inventada
            if "no se encuentra" not in ai_response.lower() and not validate_response_relevance(ai_response, query_keywords, context_keywords):
                ai_response = "Esta informaci√≥n no se encuentra claramente especificada en los documentos disponibles."
            
            if ai_response:
                # Mostrar fuentes
                sources = list(set([r['source'] for r in results]))
                
                st.markdown(
                    "<div style='background:#e3f9ed; padding:1em; border-radius:8px; border-left:4px solid #009639;'>",
                    unsafe_allow_html=True
                )
                st.markdown(f"**ü§ñ Respuesta:**")
                st.markdown(ai_response)
                st.markdown("</div>", unsafe_allow_html=True)
                
                # Mostrar fuentes
                st.markdown("---")
                st.markdown("**üìÑ Fuentes consultadas:**")
                for source in sources:
                    st.markdown(f"‚Ä¢ {source}")
                
                # Mostrar fragmentos relevantes (opcional)
                with st.expander("üîç Ver fragmentos relevantes"):
                    for i, result in enumerate(results[:2], 1):
                        st.markdown(f"**Fragmento {i}** (Score: {result['score']:.2f}) - *{result['source']}*")
                        st.markdown(f"```\n{result['content'][:300]}...\n```")
                        
        else:
            st.warning("‚ùå No se encontraron fragmentos relevantes para tu consulta.")
            st.info("üí° **Sugerencias:**\n- Usa t√©rminos m√°s espec√≠ficos\n- Verifica la ortograf√≠a\n- Intenta con sin√≥nimos")
    
else:
    st.warning("‚ö†Ô∏è No hay documentos cargados")
    st.info("""
    **Para usar el chatbot:**
    1. Crea una carpeta llamada `documents` en el mismo directorio del script
    2. Coloca tus archivos PDF de hojas de seguridad en esa carpeta
    3. Reinicia la aplicaci√≥n
    """)

# --- SIDEBAR INFO ---
with st.sidebar:
    st.markdown("### ‚ÑπÔ∏è Informaci√≥n")
    st.markdown(f"**Entorno:** {env_badge}")
    st.markdown(f"**Documentos:** {len(all_chunks)} fragmentos")
    
    if all_chunks:
        # Mostrar lista de documentos
        pdf_files = glob.glob(os.path.join(DOCUMENTS_PATH, "*.pdf"))
        st.markdown("**üìÅ Archivos cargados:**")
        for pdf_file in pdf_files:
            filename = os.path.basename(pdf_file)
            st.markdown(f"‚Ä¢ {filename}")
    
    st.markdown("---")
    st.markdown("### üîß Configuraci√≥n")
    
    if is_cloud:
        st.info("**Modo Cloud:** Usando Mistral API")
        st.markdown("Configura `MISTRAL_API_KEY` en Secrets")
    else:
        st.info("**Modo Local:** Usando llama.cpp")
        st.markdown("Servidor: http://127.0.0.1:8000")
    
    st.markdown("---")
    st.markdown("### üí° Consejos")
    st.markdown("""
    - Haz preguntas espec√≠ficas
    - Menciona el producto si es posible
    - Usa t√©rminos t√©cnicos cuando sea necesario
    - Pregunta sobre: seguridad, aplicaci√≥n, dosis, primeros auxilios
    """)